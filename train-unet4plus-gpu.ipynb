{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "# Display kaggle GPU info\n! nvidia-smi\n\nimport numpy as np\nimport tensorflow as tf\nimport keras\n\nprint(\"TensorFlow version: \" + tf.__version__)\nprint(\"Keras version: \" + keras.__version__)\nkeras.backend.set_image_data_format(\"channels_last\")",
   "metadata": {
    "_uuid": "3f29ec5a-28c1-4005-902d-7e9c525f443e",
    "_cell_guid": "45d2cbb7-fe3c-465b-a38a-837e14a73db2",
    "scrolled": true,
    "execution": {
     "iopub.status.busy": "2022-02-05T11:09:40.684225Z",
     "iopub.execute_input": "2022-02-05T11:09:40.684557Z",
     "iopub.status.idle": "2022-02-05T11:09:41.487269Z",
     "shell.execute_reply.started": "2022-02-05T11:09:40.684525Z",
     "shell.execute_reply": "2022-02-05T11:09:41.486216Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Define global variables\n",
    "# BATCH_SIZE: 一次训练所抓取的数据样本数量\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# DATASET_NAME: 数据集名称 (BickleyDiary, DIBCO, PLM)\n",
    "DATASET_NAME = \"DIBCO\"\n",
    "\n",
    "# NETWORK_MODEL: 深度网络模型名称 (UNet, UNet1Plus_w_DeepSupv, UNet1Plus_wo_DeepSupv, UNet2Plus_w_DeepSupv, UNet2Plus_wo_DeepSupv, UNet3Plus_w_DeepSupv, UNet3Plus_wo_DeepSupv, UNet4Plus_w_DeepSupv, UNet4Plus_wo_DeepSupv)\n",
    "NETWORK_MODEL = \"UNet4Plus_wo_DeepSupv\"\n",
    "\n",
    "# LOSS_FUNCTION: 深度网络损失函数 (BCE_Dice, BCE_Dice_mIoU)\n",
    "LOSS_FUNCTION = \"BCE_Dice_mIoU\"\n",
    "\n",
    "# NUM_EPOCHS: 最大训练迭代次数\n",
    "NUM_EPOCHS = 500\n",
    "\n",
    "# NUM_FILTERS: 网络第一层通道滤波器数量\n",
    "NUM_FILTERS = 32\n",
    "\n",
    "# TILE_SIZE: 图像子块大小\n",
    "TILE_SIZE = 128"
   ],
   "metadata": {
    "_uuid": "8d06ce15-dd5b-4a36-b447-0ea97c3b2e69",
    "_cell_guid": "df339a79-3e1b-45ed-ae67-74d57d9eeb22",
    "scrolled": true,
    "execution": {
     "iopub.status.busy": "2022-02-05T11:09:41.491075Z",
     "iopub.execute_input": "2022-02-05T11:09:41.492028Z",
     "iopub.status.idle": "2022-02-05T11:09:41.499673Z",
     "shell.execute_reply.started": "2022-02-05T11:09:41.491978Z",
     "shell.execute_reply": "2022-02-05T11:09:41.49768Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Brian Beck提供了一个类switch来实现switch功能\nclass switch(object):\n  def __init__(self, value):\n    self.value = value\n    self.fall = False\n  def __iter__(self):\n    \"\"\"Return the match method once, then stop\"\"\"\n    yield self.match\n    raise StopIteration\n  def match(self, *args):\n    \"\"\"Indicate whether or not to enter a case suite\"\"\"\n    if self.fall or not args:\n      return True\n    elif self.value in args: # changed for v1.5, see below\n      self.fall = True\n      return True\n    else:\n      return False",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-02-05T11:09:41.501011Z",
     "iopub.execute_input": "2022-02-05T11:09:41.501959Z",
     "iopub.status.idle": "2022-02-05T11:09:41.516605Z",
     "shell.execute_reply.started": "2022-02-05T11:09:41.501914Z",
     "shell.execute_reply": "2022-02-05T11:09:41.515447Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Create network models\nfrom keras.callbacks import *\nfrom keras.layers import *\nfrom keras.losses import *\nfrom keras.models import *\nfrom keras.optimizers import *\nfrom keras.regularizers import *\nfrom keras import backend as K\n\n\ndef conv_block(input_tensor, num_filters):\n    out = Conv2D(num_filters, kernel_size=3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\")(input_tensor)\n    out = Conv2D(num_filters, kernel_size=3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\")(out)\n    return out\n\n\ndef conv(input_tensor, num_filters):\n    out = Conv2D(num_filters, kernel_size=3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\")(input_tensor)\n    return out\n\n\ndef up_conv(input_tensor, num_filters, up_size):\n    out = UpSampling2D(up_size, interpolation=\"bilinear\")(input_tensor)\n    out = Conv2D(num_filters, kernel_size=3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\")(out)\n    return out\n\n\ndef down_conv(input_tensor, num_filters, down_size):\n    out = MaxPooling2D(down_size)(input_tensor)\n    out = Conv2D(num_filters, kernel_size=3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\")(out)\n    return out\n\n\ndef UNet(num_classes, input_height, input_width, num_filters):\n    \"\"\"\n    U-Net\n    Paper : https://arxiv.org/abs/1505.04597\n    \"\"\"\n    inputs = Input(shape=(input_height, input_width, 1))\n\n    filters = [num_filters, num_filters * 2, num_filters * 4, num_filters * 8, num_filters * 16]\n\n    e1 = conv_block(inputs, filters[0])\n\n    e2 = MaxPooling2D()(e1)\n    e2 = conv_block(e2, filters[1])\n\n    e3 = MaxPooling2D()(e2)\n    e3 = conv_block(e3, filters[2])\n\n    e4 = MaxPooling2D()(e3)\n    e4 = conv_block(e4, filters[3])\n\n    e5 = MaxPooling2D()(e4)\n    e5 = conv_block(e5, filters[4])\n\n    d4 = up_conv(e5, filters[3], 2)\n    d4 = Concatenate()([e4, d4])\n    d4 = conv_block(d4, filters[3])\n\n    d3 = up_conv(d4, filters[2], 2)\n    d3 = Concatenate()([e3, d3])\n    d3 = conv_block(d3, filters[2])\n\n    d2 = up_conv(d3, filters[1], 2)\n    d2 = Concatenate()([e2, d2])\n    d2 = conv_block(d2, filters[1])\n\n    d1 = up_conv(d2, filters[0], 2)\n    d1 = Concatenate()([e1, d1])\n    d1 = conv_block(d1, filters[0])\n\n    outputs = Conv2D(num_classes, kernel_size=3, padding=\"same\", activation=\"sigmoid\", kernel_initializer=\"he_normal\")(d1)\n\n    model = Model(inputs=inputs, outputs=outputs, name=\"UNet\")\n\n    return model\n\n\ndef UNet1Plus_w_DeepSupv(num_classes, input_height, input_width, num_filters):\n    \"\"\"\n    U-Net+ (Deep Layer Aggregation) with deep supervision\n    Paper : http://openaccess.thecvf.com/content_cvpr_2018/papers/Yu_Deep_Layer_Aggregation_CVPR_2018_paper.pdf\n    \"\"\"\n    inputs = Input(shape=(input_height, input_width, 1))\n\n    filters = [num_filters, num_filters * 2, num_filters * 4, num_filters * 8, num_filters * 16]\n\n    x0_0 = conv_block(inputs, filters[0])\n\n    x1_0 = MaxPooling2D()(x0_0)\n    x1_0 = conv_block(x1_0, filters[1])\n\n    x0_1 = up_conv(x1_0, filters[0], 2)\n    x0_1 = Concatenate()([x0_0, x0_1])\n    x0_1 = conv_block(x0_1, filters[0])\n\n    x2_0 = MaxPooling2D()(x1_0)\n    x2_0 = conv_block(x2_0, filters[2])\n\n    x1_1 = up_conv(x2_0, filters[1], 2)\n    x1_1 = Concatenate()([x1_0, x1_1])\n    x1_1 = conv_block(x1_1, filters[1])\n\n    x0_2 = up_conv(x1_1, filters[0], 2)\n    x0_2 = Concatenate()([x0_1, x0_2])\n    x0_2 = conv_block(x0_2, filters[0])\n\n    x3_0 = MaxPooling2D()(x2_0)\n    x3_0 = conv_block(x3_0, filters[3])\n\n    x2_1 = up_conv(x3_0, filters[2], 2)\n    x2_1 = Concatenate()([x2_0, x2_1])\n    x2_1 = conv_block(x2_1, filters[2])\n\n    x1_2 = up_conv(x2_1, filters[1], 2)\n    x1_2 = Concatenate()([x1_1, x1_2])\n    x1_2 = conv_block(x1_2, filters[1])\n\n    x0_3 = up_conv(x1_2, filters[0], 2)\n    x0_3 = Concatenate()([x0_2, x0_3])\n    x0_3 = conv_block(x0_3, filters[0])\n\n    x4_0 = MaxPooling2D()(x3_0)\n    x4_0 = conv_block(x4_0, filters[4])\n\n    x3_1 = up_conv(x4_0, filters[3], 2)\n    x3_1 = Concatenate()([x3_0, x3_1])\n    x3_1 = conv_block(x3_1, filters[3])\n\n    x2_2 = up_conv(x3_1, filters[2], 2)\n    x2_2 = Concatenate()([x2_1, x2_2])\n    x2_2 = conv_block(x2_2, filters[2])\n\n    x1_3 = up_conv(x2_2, filters[1], 2)\n    x1_3 = Concatenate()([x1_2, x1_3])\n    x1_3 = conv_block(x1_3, filters[1])\n\n    x0_4 = up_conv(x1_3, filters[0], 2)\n    x0_4 = Concatenate()([x0_3, x0_4])\n    x0_4 = conv_block(x0_4, filters[0])\n\n    outputs = Average()([x0_1, x0_2, x0_3, x0_4])\n    outputs = Conv2D(num_classes, kernel_size=3, padding=\"same\", activation=\"sigmoid\", kernel_initializer=\"he_normal\")(outputs)\n\n    model = Model(inputs=inputs, outputs=outputs, name=\"UNet1Plus_w_DeepSupv\")\n\n    return model\n\n\ndef UNet1Plus_wo_DeepSupv(num_classes, input_height, input_width, num_filters):\n    \"\"\"\n    U-Net+ (Deep Layer Aggregation) without deep supervision\n    Paper : http://openaccess.thecvf.com/content_cvpr_2018/papers/Yu_Deep_Layer_Aggregation_CVPR_2018_paper.pdf\n    \"\"\"\n    inputs = Input(shape=(input_height, input_width, 1))\n\n    filters = [num_filters, num_filters * 2, num_filters * 4, num_filters * 8, num_filters * 16]\n\n    x0_0 = conv_block(inputs, filters[0])\n\n    x1_0 = MaxPooling2D()(x0_0)\n    x1_0 = conv_block(x1_0, filters[1])\n\n    x0_1 = up_conv(x1_0, filters[0], 2)\n    x0_1 = Concatenate()([x0_0, x0_1])\n    x0_1 = conv_block(x0_1, filters[0])\n\n    x2_0 = MaxPooling2D()(x1_0)\n    x2_0 = conv_block(x2_0, filters[2])\n\n    x1_1 = up_conv(x2_0, filters[1], 2)\n    x1_1 = Concatenate()([x1_0, x1_1])\n    x1_1 = conv_block(x1_1, filters[1])\n\n    x0_2 = up_conv(x1_1, filters[0], 2)\n    x0_2 = Concatenate()([x0_1, x0_2])\n    x0_2 = conv_block(x0_2, filters[0])\n\n    x3_0 = MaxPooling2D()(x2_0)\n    x3_0 = conv_block(x3_0, filters[3])\n\n    x2_1 = up_conv(x3_0, filters[2], 2)\n    x2_1 = Concatenate()([x2_0, x2_1])\n    x2_1 = conv_block(x2_1, filters[2])\n\n    x1_2 = up_conv(x2_1, filters[1], 2)\n    x1_2 = Concatenate()([x1_1, x1_2])\n    x1_2 = conv_block(x1_2, filters[1])\n\n    x0_3 = up_conv(x1_2, filters[0], 2)\n    x0_3 = Concatenate()([x0_2, x0_3])\n    x0_3 = conv_block(x0_3, filters[0])\n\n    x4_0 = MaxPooling2D()(x3_0)\n    x4_0 = conv_block(x4_0, filters[4])\n\n    x3_1 = up_conv(x4_0, filters[3], 2)\n    x3_1 = Concatenate()([x3_0, x3_1])\n    x3_1 = conv_block(x3_1, filters[3])\n\n    x2_2 = up_conv(x3_1, filters[2], 2)\n    x2_2 = Concatenate()([x2_1, x2_2])\n    x2_2 = conv_block(x2_2, filters[2])\n\n    x1_3 = up_conv(x2_2, filters[1], 2)\n    x1_3 = Concatenate()([x1_2, x1_3])\n    x1_3 = conv_block(x1_3, filters[1])\n\n    x0_4 = up_conv(x1_3, filters[0], 2)\n    x0_4 = Concatenate()([x0_3, x0_4])\n    x0_4 = conv_block(x0_4, filters[0])\n\n    outputs = Conv2D(num_classes, kernel_size=3, padding=\"same\", activation=\"sigmoid\", kernel_initializer=\"he_normal\")(x0_4)\n\n    model = Model(inputs=inputs, outputs=outputs, name=\"UNet1Plus_wo_DeepSupv\")\n\n    return model\n\n\ndef UNet2Plus_w_DeepSupv(num_classes, input_height, input_width, num_filters):\n    \"\"\"\n    U-Net++ with deep supervision\n    Paper : https://arxiv.org/abs/1807.10165\n    \"\"\"\n    inputs = Input(shape=(input_height, input_width, 1))\n\n    filters = [num_filters, num_filters * 2, num_filters * 4, num_filters * 8, num_filters * 16]\n\n    x0_0 = conv_block(inputs, filters[0])\n\n    x1_0 = MaxPooling2D()(x0_0)\n    x1_0 = conv_block(x1_0, filters[1])\n\n    x0_1 = up_conv(x1_0, filters[0], 2)\n    x0_1 = Concatenate()([x0_0, x0_1])\n    x0_1 = conv_block(x0_1, filters[0])\n\n    x2_0 = MaxPooling2D()(x1_0)\n    x2_0 = conv_block(x2_0, filters[2])\n\n    x1_1 = up_conv(x2_0, filters[1], 2)\n    x1_1 = Concatenate()([x1_0, x1_1])\n    x1_1 = conv_block(x1_1, filters[1])\n\n    x0_2 = up_conv(x1_1, filters[0], 2)\n    x0_2 = Concatenate()([x0_0, x0_1, x0_2])\n    x0_2 = conv_block(x0_2, filters[0])\n\n    x3_0 = MaxPooling2D()(x2_0)\n    x3_0 = conv_block(x3_0, filters[3])\n\n    x2_1 = up_conv(x3_0, filters[2], 2)\n    x2_1 = Concatenate()([x2_0, x2_1])\n    x2_1 = conv_block(x2_1, filters[2])\n\n    x1_2 = up_conv(x2_1, filters[1], 2)\n    x1_2 = Concatenate()([x1_0, x1_1, x1_2])\n    x1_2 = conv_block(x1_2, filters[1])\n\n    x0_3 = up_conv(x1_2, filters[0], 2)\n    x0_3 = Concatenate()([x0_0, x0_1, x0_2, x0_3])\n    x0_3 = conv_block(x0_3, filters[0])\n\n    x4_0 = MaxPooling2D()(x3_0)\n    x4_0 = conv_block(x4_0, filters[4])\n\n    x3_1 = up_conv(x4_0, filters[3], 2)\n    x3_1 = Concatenate()([x3_0, x3_1])\n    x3_1 = conv_block(x3_1, filters[3])\n\n    x2_2 = up_conv(x3_1, filters[2], 2)\n    x2_2 = Concatenate()([x2_0, x2_1, x2_2])\n    x2_2 = conv_block(x2_2, filters[2])\n\n    x1_3 = up_conv(x2_2, filters[1], 2)\n    x1_3 = Concatenate()([x1_0, x1_1, x1_2, x1_3])\n    x1_3 = conv_block(x1_3, filters[1])\n\n    x0_4 = up_conv(x1_3, filters[0], 2)\n    x0_4 = Concatenate()([x0_0, x0_1, x0_2, x0_3, x0_4])\n    x0_4 = conv_block(x0_4, filters[0])\n\n    outputs = Average()([x0_1, x0_2, x0_3, x0_4])\n    outputs = Conv2D(num_classes, kernel_size=3, padding=\"same\", activation=\"sigmoid\", kernel_initializer=\"he_normal\")(outputs)\n\n    model = Model(inputs=inputs, outputs=outputs, name=\"UNet2Plus_w_DeepSupv\")\n\n    return model\n\n\ndef UNet2Plus_wo_DeepSupv(num_classes, input_height, input_width, num_filters):\n    \"\"\"\n    U-Net++ without deep supervision\n    Paper : https://arxiv.org/abs/1807.10165\n    \"\"\"\n    inputs = Input(shape=(input_height, input_width, 1))\n\n    filters = [num_filters, num_filters * 2, num_filters * 4, num_filters * 8, num_filters * 16]\n\n    x0_0 = conv_block(inputs, filters[0])\n\n    x1_0 = MaxPooling2D()(x0_0)\n    x1_0 = conv_block(x1_0, filters[1])\n\n    x0_1 = up_conv(x1_0, filters[0], 2)\n    x0_1 = Concatenate()([x0_0, x0_1])\n    x0_1 = conv_block(x0_1, filters[0])\n\n    x2_0 = MaxPooling2D()(x1_0)\n    x2_0 = conv_block(x2_0, filters[2])\n\n    x1_1 = up_conv(x2_0, filters[1], 2)\n    x1_1 = Concatenate()([x1_0, x1_1])\n    x1_1 = conv_block(x1_1, filters[1])\n\n    x0_2 = up_conv(x1_1, filters[0], 2)\n    x0_2 = Concatenate()([x0_0, x0_1, x0_2])\n    x0_2 = conv_block(x0_2, filters[0])\n\n    x3_0 = MaxPooling2D()(x2_0)\n    x3_0 = conv_block(x3_0, filters[3])\n\n    x2_1 = up_conv(x3_0, filters[2], 2)\n    x2_1 = Concatenate()([x2_0, x2_1])\n    x2_1 = conv_block(x2_1, filters[2])\n\n    x1_2 = up_conv(x2_1, filters[1], 2)\n    x1_2 = Concatenate()([x1_0, x1_1, x1_2])\n    x1_2 = conv_block(x1_2, filters[1])\n\n    x0_3 = up_conv(x1_2, filters[0], 2)\n    x0_3 = Concatenate()([x0_0, x0_1, x0_2, x0_3])\n    x0_3 = conv_block(x0_3, filters[0])\n\n    x4_0 = MaxPooling2D()(x3_0)\n    x4_0 = conv_block(x4_0, filters[4])\n\n    x3_1 = up_conv(x4_0, filters[3], 2)\n    x3_1 = Concatenate()([x3_0, x3_1])\n    x3_1 = conv_block(x3_1, filters[3])\n\n    x2_2 = up_conv(x3_1, filters[2], 2)\n    x2_2 = Concatenate()([x2_0, x2_1, x2_2])\n    x2_2 = conv_block(x2_2, filters[2])\n\n    x1_3 = up_conv(x2_2, filters[1], 2)\n    x1_3 = Concatenate()([x1_0, x1_1, x1_2, x1_3])\n    x1_3 = conv_block(x1_3, filters[1])\n\n    x0_4 = up_conv(x1_3, filters[0], 2)\n    x0_4 = Concatenate()([x0_0, x0_1, x0_2, x0_3, x0_4])\n    x0_4 = conv_block(x0_4, filters[0])\n\n    outputs = Conv2D(num_classes, kernel_size=3, padding=\"same\", activation=\"sigmoid\", kernel_initializer=\"he_normal\")(x0_4)\n\n    model = Model(inputs=inputs, outputs=outputs, name=\"UNet2Plus_wo_DeepSupv\")\n\n    return model\n\n\ndef UNet3Plus_w_DeepSupv(num_classes, input_height, input_width, num_filters):\n    \"\"\"\n    U-Net3+ with deep supervision\n    Paper : https://arxiv.org/abs/2004.08790\n    \"\"\"\n    inputs = Input(shape=(input_height, input_width, 1))\n\n    filters = [num_filters, num_filters * 2, num_filters * 4, num_filters * 8, num_filters * 16]\n\n    e1 = conv_block(inputs, filters[0])\n\n    e2 = MaxPooling2D()(e1)\n    e2 = conv_block(e2, filters[1])\n\n    e3 = MaxPooling2D()(e2)\n    e3 = conv_block(e3, filters[2])\n\n    e4 = MaxPooling2D()(e3)\n    e4 = conv_block(e4, filters[3])\n\n    e5 = MaxPooling2D()(e4)\n    e5 = conv_block(e5, filters[4])\n\n    e1_d_d4 = down_conv(e1, filters[0], 8)\n    e2_d_d4 = down_conv(e2, filters[0], 4)\n    e3_d_d4 = down_conv(e3, filters[0], 2)\n    e4_d4 = conv(e4, filters[0])\n    e5_u_d4 = up_conv(e5, filters[0], 2)\n    d4 = Concatenate()([e1_d_d4, e2_d_d4, e3_d_d4, e4_d4, e5_u_d4])\n    d4 = conv_block(d4, filters[0] * 5)\n\n    e1_d_d3 = down_conv(e1, filters[0], 4)\n    e2_d_d3 = down_conv(e2, filters[0], 2)\n    e3_d3 = conv(e3, filters[0])\n    e5_u_d3 = up_conv(e5, filters[0], 4)\n    d4_u_d3 = up_conv(d4, filters[0], 2)\n    d3 = Concatenate()([e1_d_d3, e2_d_d3, e3_d3, e5_u_d3, d4_u_d3])\n    d3 = conv_block(d3, filters[0] * 5)\n\n    e1_d_d2 = down_conv(e1, filters[0], 2)\n    e2_d2 = conv(e2, filters[0])\n    e5_u_d2 = up_conv(e5, filters[0], 8)\n    d4_u_d2 = up_conv(d4, filters[0], 4)\n    d3_u_d2 = up_conv(d3, filters[0], 2)\n    d2 = Concatenate()([e1_d_d2, e2_d2, e5_u_d2, d4_u_d2, d3_u_d2])\n    d2 = conv_block(d2, filters[0] * 5)\n\n    e1_d1 = conv(e1, filters[0])\n    e5_u_d1 = up_conv(e5, filters[0], 16)\n    d4_u_d1 = up_conv(d4, filters[0], 8)\n    d3_u_d1 = up_conv(d3, filters[0], 4)\n    d2_u_d1 = up_conv(d2, filters[0], 2)\n    d1 = Concatenate()([e1_d1, e5_u_d1, d4_u_d1, d3_u_d1, d2_u_d1])\n    d1 = conv_block(d1, filters[0] * 5)\n\n    outputs = Average()([d1, up_conv(d2, filters[0] * 5, 2), up_conv(d3, filters[0] * 5, 4), up_conv(d4, filters[0] * 5, 8)])\n    outputs = Conv2D(num_classes, kernel_size=3, padding=\"same\", activation=\"sigmoid\", kernel_initializer=\"he_normal\")(outputs)\n\n    model = Model(inputs=inputs, outputs=outputs, name=\"UNet3Plus_w_DeepSupv\")\n\n    return model\n\n\ndef UNet3Plus_wo_DeepSupv(num_classes, input_height, input_width, num_filters):\n    \"\"\"\n    U-Net3+ without deep supervision\n    Paper : https://arxiv.org/abs/2004.08790\n    \"\"\"\n    inputs = Input(shape=(input_height, input_width, 1))\n\n    filters = [num_filters, num_filters * 2, num_filters * 4, num_filters * 8, num_filters * 16]\n\n    e1 = conv_block(inputs, filters[0])\n\n    e2 = MaxPooling2D()(e1)\n    e2 = conv_block(e2, filters[1])\n\n    e3 = MaxPooling2D()(e2)\n    e3 = conv_block(e3, filters[2])\n\n    e4 = MaxPooling2D()(e3)\n    e4 = conv_block(e4, filters[3])\n\n    e5 = MaxPooling2D()(e4)\n    e5 = conv_block(e5, filters[4])\n\n    e1_d_d4 = down_conv(e1, filters[0], 8)\n    e2_d_d4 = down_conv(e2, filters[0], 4)\n    e3_d_d4 = down_conv(e3, filters[0], 2)\n    e4_d4 = conv(e4, filters[0])\n    e5_u_d4 = up_conv(e5, filters[0], 2)\n    d4 = Concatenate()([e1_d_d4, e2_d_d4, e3_d_d4, e4_d4, e5_u_d4])\n    d4 = conv_block(d4, filters[0] * 5)\n\n    e1_d_d3 = down_conv(e1, filters[0], 4)\n    e2_d_d3 = down_conv(e2, filters[0], 2)\n    e3_d3 = conv(e3, filters[0])\n    e5_u_d3 = up_conv(e5, filters[0], 4)\n    d4_u_d3 = up_conv(d4, filters[0], 2)\n    d3 = Concatenate()([e1_d_d3, e2_d_d3, e3_d3, e5_u_d3, d4_u_d3])\n    d3 = conv_block(d3, filters[0] * 5)\n\n    e1_d_d2 = down_conv(e1, filters[0], 2)\n    e2_d2 = conv(e2, filters[0])\n    e5_u_d2 = up_conv(e5, filters[0], 8)\n    d4_u_d2 = up_conv(d4, filters[0], 4)\n    d3_u_d2 = up_conv(d3, filters[0], 2)\n    d2 = Concatenate()([e1_d_d2, e2_d2, e5_u_d2, d4_u_d2, d3_u_d2])\n    d2 = conv_block(d2, filters[0] * 5)\n\n    e1_d1 = conv(e1, filters[0])\n    e5_u_d1 = up_conv(e5, filters[0], 16)\n    d4_u_d1 = up_conv(d4, filters[0], 8)\n    d3_u_d1 = up_conv(d3, filters[0], 4)\n    d2_u_d1 = up_conv(d2, filters[0], 2)\n    d1 = Concatenate()([e1_d1, e5_u_d1, d4_u_d1, d3_u_d1, d2_u_d1])\n    d1 = conv_block(d1, filters[0] * 5)\n\n    outputs = Conv2D(num_classes, kernel_size=3, padding=\"same\", activation=\"sigmoid\", kernel_initializer=\"he_normal\")(d1)\n\n    model = Model(inputs=inputs, outputs=outputs, name=\"UNet3Plus_wo_DeepSupv\")\n\n    return model\n\n\ndef UNet4Plus_w_DeepSupv(num_classes, input_height, input_width, num_filters):\n    \"\"\"\n    U-Net4+ with deep supervision\n    \"\"\"\n    inputs = Input(shape=(input_height, input_width, 1))\n\n    e1 = conv_block(inputs, num_filters)\n\n    e2 = down_conv(e1, num_filters, 2)\n    e2 = conv_block(e2, num_filters)\n\n    e1_d_e3 = down_conv(e1, num_filters, 4)\n    e2_d_e3 = down_conv(e2, num_filters, 2)\n    e3 = Concatenate()([e1_d_e3, e2_d_e3])\n    e3 = conv_block(e3, num_filters * 2)\n\n    e1_d_e4 = down_conv(e1, num_filters, 8)\n    e2_d_e4 = down_conv(e2, num_filters, 4)\n    e3_d_e4 = down_conv(e3, num_filters, 2)\n    e4 = Concatenate()([e1_d_e4, e2_d_e4, e3_d_e4])\n    e4 = conv_block(e4, num_filters * 3)\n\n    e1_d_e5 = down_conv(e1, num_filters, 16)\n    e2_d_e5 = down_conv(e2, num_filters, 8)\n    e3_d_e5 = down_conv(e3, num_filters, 4)\n    e4_d_e5 = down_conv(e4, num_filters, 2)\n    e5 = Concatenate()([e1_d_e5, e2_d_e5, e3_d_e5, e4_d_e5])\n    e5 = conv_block(e5, num_filters * 4)\n\n    e1_d_d4 = down_conv(e1, num_filters, 8)\n    e2_d_d4 = down_conv(e2, num_filters, 4)\n    e3_d_d4 = down_conv(e3, num_filters, 2)\n    e4_d4 = conv(e4, num_filters)\n    e5_u_d4 = up_conv(e5, num_filters, 2)\n    d4 = Concatenate()([e1_d_d4, e2_d_d4, e3_d_d4, e4_d4, e5_u_d4])\n    d4 = conv_block(d4, num_filters * 5)\n\n    e1_d_d3 = down_conv(e1, num_filters, 4)\n    e2_d_d3 = down_conv(e2, num_filters, 2)\n    e3_d3 = conv(e3, num_filters)\n    e4_u_d3 = up_conv(e4, num_filters, 2)\n    e5_u_d3 = up_conv(e5, num_filters, 4)\n    d4_u_d3 = up_conv(d4, num_filters, 2)\n    d3 = Concatenate()([e1_d_d3, e2_d_d3, e3_d3, e4_u_d3, e5_u_d3, d4_u_d3])\n    d3 = conv_block(d3, num_filters * 6)\n\n    e1_d_d2 = down_conv(e1, num_filters, 2)\n    e2_d2 = conv(e2, num_filters)\n    e3_u_d2 = up_conv(e3, num_filters, 2)\n    e4_u_d2 = up_conv(e4, num_filters, 4)\n    e5_u_d2 = up_conv(e5, num_filters, 8)\n    d4_u_d2 = up_conv(d4, num_filters, 4)\n    d3_u_d2 = up_conv(d3, num_filters, 2)\n    d2 = Concatenate()([e1_d_d2, e2_d2, e3_u_d2, e4_u_d2, e5_u_d2, d4_u_d2, d3_u_d2])\n    d2 = conv_block(d2, num_filters * 7)\n\n    e1_d1 = conv(e1, num_filters)\n    e2_u_d1 = up_conv(e2, num_filters, 2)\n    e3_u_d1 = up_conv(e3, num_filters, 4)\n    e4_u_d1 = up_conv(e4, num_filters, 8)\n    e5_u_d1 = up_conv(e5, num_filters, 16)\n    d4_u_d1 = up_conv(d4, num_filters, 8)\n    d3_u_d1 = up_conv(d3, num_filters, 4)\n    d2_u_d1 = up_conv(d2, num_filters, 2)\n    d1 = Concatenate()([e1_d1, e2_u_d1, e3_u_d1, e4_u_d1, e5_u_d1, d4_u_d1, d3_u_d1, d2_u_d1])\n    d1 = conv_block(d1, num_filters * 8)\n\n    outputs = Concatenate()([d1,\n                             UpSampling2D(size=2, interpolation=\"bilinear\")(d2),\n                             UpSampling2D(size=4, interpolation=\"bilinear\")(d3),\n                             UpSampling2D(size=8, interpolation=\"bilinear\")(d4)])\n    outputs = Conv2D(num_classes, kernel_size=3, padding=\"same\", activation=\"sigmoid\", kernel_initializer=\"he_normal\")(outputs)\n\n    model = Model(inputs=inputs, outputs=outputs, name=\"UNet4Plus_w_DeepSupv\")\n\n    return model\n\n\ndef UNet4Plus_wo_DeepSupv(num_classes, input_height, input_width, num_filters):\n    \"\"\"\n    U-Net4+ without deep supervision\n    \"\"\"\n    inputs = Input(shape=(input_height, input_width, 1))\n\n    e1 = conv_block(inputs, num_filters)\n\n    e2 = down_conv(e1, num_filters, 2)\n    e2 = conv_block(e2, num_filters)\n\n    e1_d_e3 = down_conv(e1, num_filters, 4)\n    e2_d_e3 = down_conv(e2, num_filters, 2)\n    e3 = Concatenate()([e1_d_e3, e2_d_e3])\n    e3 = conv_block(e3, num_filters * 2)\n\n    e1_d_e4 = down_conv(e1, num_filters, 8)\n    e2_d_e4 = down_conv(e2, num_filters, 4)\n    e3_d_e4 = down_conv(e3, num_filters, 2)\n    e4 = Concatenate()([e1_d_e4, e2_d_e4, e3_d_e4])\n    e4 = conv_block(e4, num_filters * 3)\n\n    e1_d_e5 = down_conv(e1, num_filters, 16)\n    e2_d_e5 = down_conv(e2, num_filters, 8)\n    e3_d_e5 = down_conv(e3, num_filters, 4)\n    e4_d_e5 = down_conv(e4, num_filters, 2)\n    e5 = Concatenate()([e1_d_e5, e2_d_e5, e3_d_e5, e4_d_e5])\n    e5 = conv_block(e5, num_filters * 4)\n\n    e1_d_d4 = down_conv(e1, num_filters, 8)\n    e2_d_d4 = down_conv(e2, num_filters, 4)\n    e3_d_d4 = down_conv(e3, num_filters, 2)\n    e4_d4 = conv(e4, num_filters)\n    e5_u_d4 = up_conv(e5, num_filters, 2)\n    d4 = Concatenate()([e1_d_d4, e2_d_d4, e3_d_d4, e4_d4, e5_u_d4])\n    d4 = conv_block(d4, num_filters * 5)\n\n    e1_d_d3 = down_conv(e1, num_filters, 4)\n    e2_d_d3 = down_conv(e2, num_filters, 2)\n    e3_d3 = conv(e3, num_filters)\n    e4_u_d3 = up_conv(e4, num_filters, 2)\n    e5_u_d3 = up_conv(e5, num_filters, 4)\n    d4_u_d3 = up_conv(d4, num_filters, 2)\n    d3 = Concatenate()([e1_d_d3, e2_d_d3, e3_d3, e4_u_d3, e5_u_d3, d4_u_d3])\n    d3 = conv_block(d3, num_filters * 6)\n\n    e1_d_d2 = down_conv(e1, num_filters, 2)\n    e2_d2 = conv(e2, num_filters)\n    e3_u_d2 = up_conv(e3, num_filters, 2)\n    e4_u_d2 = up_conv(e4, num_filters, 4)\n    e5_u_d2 = up_conv(e5, num_filters, 8)\n    d4_u_d2 = up_conv(d4, num_filters, 4)\n    d3_u_d2 = up_conv(d3, num_filters, 2)\n    d2 = Concatenate()([e1_d_d2, e2_d2, e3_u_d2, e4_u_d2, e5_u_d2, d4_u_d2, d3_u_d2])\n    d2 = conv_block(d2, num_filters * 7)\n\n    e1_d1 = conv(e1, num_filters)\n    e2_u_d1 = up_conv(e2, num_filters, 2)\n    e3_u_d1 = up_conv(e3, num_filters, 4)\n    e4_u_d1 = up_conv(e4, num_filters, 8)\n    e5_u_d1 = up_conv(e5, num_filters, 16)\n    d4_u_d1 = up_conv(d4, num_filters, 8)\n    d3_u_d1 = up_conv(d3, num_filters, 4)\n    d2_u_d1 = up_conv(d2, num_filters, 2)\n    d1 = Concatenate()([e1_d1, e2_u_d1, e3_u_d1, e4_u_d1, e5_u_d1, d4_u_d1, d3_u_d1, d2_u_d1])\n    d1 = conv_block(d1, num_filters * 8)\n\n    outputs = Conv2D(num_classes, kernel_size=3, padding=\"same\", activation=\"sigmoid\", kernel_initializer=\"he_normal\")(d1)\n\n    model = Model(inputs=inputs, outputs=outputs, name=\"UNet4Plus_wo_DeepSupv\")\n\n    return model",
   "metadata": {
    "_uuid": "7c546016-925e-4ed6-a6ac-bb18ff2ae3d1",
    "_cell_guid": "a4bec866-f05b-44d0-abff-a2599d78a9a1",
    "scrolled": true,
    "execution": {
     "iopub.status.busy": "2022-02-05T11:09:41.519677Z",
     "iopub.execute_input": "2022-02-05T11:09:41.520469Z",
     "iopub.status.idle": "2022-02-05T11:09:41.669023Z",
     "shell.execute_reply.started": "2022-02-05T11:09:41.520347Z",
     "shell.execute_reply": "2022-02-05T11:09:41.667583Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Define loss functions\nfrom skimage.morphology import label\n\n\ndef iou_metric(y_true_in, y_pred_in, print_table=False):\n    y_true = label(y_true_in > 0.5)\n    y_pred = label(y_pred_in > 0.5)\n\n    true_objects = len(np.unique(y_true))\n    pred_objects = len(np.unique(y_pred))\n\n    intersection = np.histogram2d(y_true.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))[0]\n\n    # Compute areas (needed for finding the union between all objects)\n    area_true = np.histogram(y_true, bins=true_objects)[0]\n    area_pred = np.histogram(y_pred, bins=pred_objects)[0]\n    area_true = np.expand_dims(area_true, -1)\n    area_pred = np.expand_dims(area_pred, 0)\n\n    # Compute union\n    union = area_true + area_pred - intersection\n\n    # Exclude background from the analysis\n    intersection = intersection[1:, 1:]\n    union = union[1:, 1:]\n    union[union == 0] = 1e-9\n\n    # Compute the intersection over union\n    iou = intersection / union\n\n    # print('IOU {}'.format(iou))\n    # Precision helper function\n    def precision_at(threshold, iou):\n        matches = iou > threshold\n        true_positives = np.sum(matches, axis=1) == 1  # Correct objects\n        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n        return tp, fp, fn\n\n    # Loop over IoU thresholds\n    prec = []\n    if print_table:\n        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n\n    for t in np.arange(0.5, 1.0, 0.05):\n        tp, fp, fn = precision_at(t, iou)\n        if (tp + fp + fn) > 0:\n            p = tp / (tp + fp + fn)\n        else:\n            p = 0\n        if print_table:\n            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n        prec.append(p)\n\n    if print_table:\n        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n    return np.mean(prec)\n\n\ndef iou_metric_batch(y_true, y_pred):\n    batch_size = y_true.shape[0]\n    metric = []\n    for batch in range(batch_size):\n        value = iou_metric(y_true[batch], y_pred[batch])\n        metric.append(value)\n    return np.array(np.mean(metric), dtype=np.float32)\n\n\ndef mean_iou_metric(y_true, y_pred):\n    metric_value = tf.compat.v1.py_func(iou_metric_batch, [y_true, y_pred], tf.float32)\n    return metric_value\n\n\ndef mean_iou_metric_loss(y_true, y_pred):\n    loss = 1 - mean_iou_metric(y_true, y_pred)\n    loss.set_shape((None,))\n    return loss\n\n\ndef dice_coef(y_true, y_pred, smooth=1e-6):\n    \"\"\"\n    Dice = (2*|X & Y|)/ (|X|+ |Y|)\n         =  2*sum(|A*B|)/(sum(A^2)+sum(B^2))\n    ref: https://arxiv.org/pdf/1606.04797v1.pdf\n    \"\"\"\n    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n    return (2.0 * intersection + smooth) / (K.sum(K.square(y_true), -1) + K.sum(K.square(y_pred), -1) + smooth)\n\n\ndef dice_coef_loss(y_true, y_pred):\n    return 1 - dice_coef(y_true, y_pred)\n\n\ndef _BCE_Dice_mIoU_Loss(y_true, y_pred):\n    \"\"\"\n    The segmentation loss is optimized as the weighted average of binary crossentropy,\n    dice coefficient and mean intersection over union (IoU) which is evaluated with\n    pixel accuracy, loss value and IoU. The IoU score calculation/implementation is\n    as per the Kaggle Data Science Bowl Challenge 2018 (KDSB18), which is the more\n    precise and accurate approach for computing IoU.\n\n    :param y_true: the ground truth\n    :param y_pred: the predicted\n    :return: the weighted average loss value\n    \"\"\"\n    loss = 0.4 * binary_crossentropy(y_true, y_pred) + \\\n           0.2 * dice_coef_loss(y_true, y_pred) + \\\n           0.4 * mean_iou_metric_loss(y_true, y_pred)\n    return loss\n\n\ndef BCE_Dice_mIoU_Loss():\n    return _BCE_Dice_mIoU_Loss",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-02-05T11:09:41.671794Z",
     "iopub.execute_input": "2022-02-05T11:09:41.672496Z",
     "iopub.status.idle": "2022-02-05T11:09:41.698628Z",
     "shell.execute_reply.started": "2022-02-05T11:09:41.672435Z",
     "shell.execute_reply": "2022-02-05T11:09:41.697577Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Train deep neural network model\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from datetime import datetime\n",
    "from keras.preprocessing.image import *\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "for _ in range(1):\n",
    "    K.clear_session()\n",
    "    \n",
    "    data_root = os.path.join(\"../input/dibco-training-set-128x128/dibco_train_0.8_val_0.2_seed_8281\")\n",
    "    print(\"Training {} dataset, using {} model with {} loss ...\".format(DATASET_NAME, NETWORK_MODEL, LOSS_FUNCTION))\n",
    "    \n",
    "    train_val_split_str = data_root[data_root.index(\"_\") + 1:data_root.index(\"_\") + 18]  # extract substring: train_0.8_val_0.2\n",
    "    \n",
    "    data_train_dir = os.path.join(data_root, \"train\")\n",
    "    data_val_dir = os.path.join(data_root, \"val\")\n",
    "\n",
    "    random.seed()\n",
    "    seed1 = round(random.random() * 10000)\n",
    "    seed2 = round(random.random() * 10000)\n",
    "    print(\"Random seed1: {}, and random seed2: {}\".format(seed1, seed2))\n",
    "\n",
    "    train_img_datagen = ImageDataGenerator(rescale=1.0 / 255.0)\n",
    "    train_msk_datagen = ImageDataGenerator(rescale=1.0 / 255.0)\n",
    "\n",
    "    train_img_generator = train_img_datagen.flow_from_directory(\n",
    "        data_train_dir,\n",
    "        target_size=(TILE_SIZE, TILE_SIZE),\n",
    "        color_mode=\"grayscale\",\n",
    "        classes=[\"images\"],\n",
    "        class_mode=None,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        seed=seed1,\n",
    "    )\n",
    "\n",
    "    train_msk_generator = train_msk_datagen.flow_from_directory(\n",
    "        data_train_dir,\n",
    "        target_size=(TILE_SIZE, TILE_SIZE),\n",
    "        color_mode=\"grayscale\",\n",
    "        classes=[\"labels\"],\n",
    "        class_mode=None,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        seed=seed1,\n",
    "    )\n",
    "\n",
    "    val_img_datagen = ImageDataGenerator(rescale=1.0 / 255.0)\n",
    "    val_msk_datagen = ImageDataGenerator(rescale=1.0 / 255.0)\n",
    "\n",
    "    val_img_generator = val_img_datagen.flow_from_directory(\n",
    "        data_val_dir,\n",
    "        target_size=(TILE_SIZE, TILE_SIZE),\n",
    "        color_mode=\"grayscale\",\n",
    "        classes=[\"images\"],\n",
    "        class_mode=None,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        seed=seed2,\n",
    "    )\n",
    "\n",
    "    val_msk_generator = val_msk_datagen.flow_from_directory(\n",
    "        data_val_dir,\n",
    "        target_size=(TILE_SIZE, TILE_SIZE),\n",
    "        color_mode=\"grayscale\",\n",
    "        classes=[\"labels\"],\n",
    "        class_mode=None,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        seed=seed2,\n",
    "    )\n",
    "\n",
    "    train_generator = zip(train_img_generator, train_msk_generator)\n",
    "    val_generator = zip(val_img_generator, val_msk_generator)\n",
    "    \n",
    "    for case in switch(NETWORK_MODEL):\n",
    "      if case(\"UNet\"):\n",
    "        model = UNet(num_classes=1, input_height=TILE_SIZE, input_width=TILE_SIZE, num_filters=NUM_FILTERS)\n",
    "        break\n",
    "      if case(\"UNet1Plus_w_DeepSupv\"):\n",
    "        model = UNet1Plus_w_DeepSupv(num_classes=1, input_height=TILE_SIZE, input_width=TILE_SIZE, num_filters=NUM_FILTERS)\n",
    "        break\n",
    "      if case(\"UNet1Plus_wo_DeepSupv\"):\n",
    "        model = UNet1Plus_wo_DeepSupv(num_classes=1, input_height=TILE_SIZE, input_width=TILE_SIZE, num_filters=NUM_FILTERS)\n",
    "        break\n",
    "      if case(\"UNet2Plus_w_DeepSupv\"):\n",
    "        model = UNet2Plus_w_DeepSupv(num_classes=1, input_height=TILE_SIZE, input_width=TILE_SIZE, num_filters=NUM_FILTERS)\n",
    "        break\n",
    "      if case(\"UNet2Plus_wo_DeepSupv\"):\n",
    "        model = UNet2Plus_wo_DeepSupv(num_classes=1, input_height=TILE_SIZE, input_width=TILE_SIZE, num_filters=NUM_FILTERS)\n",
    "        break\n",
    "      if case(\"UNet3Plus_w_DeepSupv\"):\n",
    "        model = UNet3Plus_w_DeepSupv(num_classes=1, input_height=TILE_SIZE, input_width=TILE_SIZE, num_filters=NUM_FILTERS)\n",
    "        break\n",
    "      if case(\"UNet3Plus_wo_DeepSupv\"):\n",
    "        model = UNet3Plus_wo_DeepSupv(num_classes=1, input_height=TILE_SIZE, input_width=TILE_SIZE, num_filters=NUM_FILTERS)\n",
    "        break\n",
    "      if case(\"UNet4Plus_w_DeepSupv\"):\n",
    "        model = UNet4Plus_w_DeepSupv(num_classes=1, input_height=TILE_SIZE, input_width=TILE_SIZE, num_filters=NUM_FILTERS)\n",
    "        break\n",
    "      if case(\"UNet4Plus_wo_DeepSupv\"):\n",
    "        model = UNet4Plus_wo_DeepSupv(num_classes=1, input_height=TILE_SIZE, input_width=TILE_SIZE, num_filters=NUM_FILTERS)\n",
    "        break\n",
    "      if case(): # default, could also just omit condition or 'if True'\n",
    "        print(\"Oops! Network Model Should Be Something Else!\")\n",
    "        # No need to break here, it'll stop anyway\n",
    "    \n",
    "    for case in switch(LOSS_FUNCTION):\n",
    "      if case(\"BCE_Dice_mIoU\"):\n",
    "        model.compile(optimizer=\"Adam\", loss=BCE_Dice_mIoU_Loss(), metrics=[\"accuracy\"])\n",
    "        break\n",
    "      if case(): # default, could also just omit condition or 'if True'\n",
    "        print(\"Oops! Loss Function Should Be Something Else!\")\n",
    "        # No need to break here, it'll stop anyway\n",
    "    \n",
    "    # model.load_weights(\"...\")\n",
    "    # model.summary()\n",
    "    \n",
    "    model_weights_root = \"./weights-\" + DATASET_NAME.lower() + \"-\" + NETWORK_MODEL.lower() + \"-\" + LOSS_FUNCTION.lower() + \"-\" + train_val_split_str + \"-\" + str(datetime.timestamp(datetime.now()))\n",
    "    if not os.path.exists(model_weights_root):\n",
    "        os.makedirs(model_weights_root)\n",
    "\n",
    "    check_point = ModelCheckpoint(os.path.join(model_weights_root,\n",
    "                                               DATASET_NAME.lower() + \"-\" + NETWORK_MODEL.lower() + \"-\" + LOSS_FUNCTION.lower() + \"-\" + train_val_split_str +\n",
    "                                               \"-ps_\" + str(TILE_SIZE) + \"x\" + str(TILE_SIZE) +\n",
    "                                               \"-ch_\" + str(NUM_FILTERS) +\n",
    "                                               \"-bs_\" + str(BATCH_SIZE) +\n",
    "                                               \"-val_loss_{val_loss}-val_accuracy_{val_accuracy}.hdf5\"),\n",
    "                                  monitor=\"val_loss\",\n",
    "                                  verbose=1,\n",
    "                                  save_best_only=True,\n",
    "                                  save_weights_only=True,\n",
    "                                  mode=\"auto\")\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\",\n",
    "                                  factor=0.5,\n",
    "                                  patience=10,\n",
    "                                  verbose=1,\n",
    "                                  mode=\"auto\")\n",
    "\n",
    "    early_stop = EarlyStopping(monitor=\"val_loss\",\n",
    "                               patience=15,\n",
    "                               verbose=1,\n",
    "                               mode=\"auto\")\n",
    "\n",
    "    print(\"Now start training the network model...\")\n",
    "    history = model.fit_generator(train_generator,\n",
    "                                  epochs=NUM_EPOCHS,\n",
    "                                  verbose=1,\n",
    "                                  steps_per_epoch=len(train_img_generator),\n",
    "                                  validation_data=val_generator,\n",
    "                                  validation_steps=len(val_img_generator),\n",
    "                                  callbacks=[check_point, reduce_lr, early_stop])\n",
    "\n",
    "    with open(os.path.join(model_weights_root,\n",
    "                           DATASET_NAME.lower() + \"-\" + NETWORK_MODEL.lower() + \"-\" + LOSS_FUNCTION.lower() + \"-\" + train_val_split_str +\n",
    "                           \"-ps_\" + str(TILE_SIZE) + \"x\" + str(TILE_SIZE) +\n",
    "                           \"-ch_\" + str(NUM_FILTERS) +\n",
    "                           \"-bs_\" + str(BATCH_SIZE) +\n",
    "                           \"-\" + str(datetime.timestamp(datetime.now())) + \".history\"), \"wb\") as df:\n",
    "        pickle.dump(history.history, df)\n",
    "\n",
    "    df.close()\n",
    "\n",
    "print(\"Finished!\")"
   ],
   "metadata": {
    "_uuid": "8d6df931-46b4-4b35-b0dc-840ea2db6ccd",
    "_cell_guid": "4daaa6a1-e579-40e6-b0c5-d02aeeb02aef",
    "execution": {
     "iopub.status.busy": "2022-02-05T11:09:41.70094Z",
     "iopub.execute_input": "2022-02-05T11:09:41.701514Z",
     "iopub.status.idle": "2022-02-05T11:12:32.807975Z",
     "shell.execute_reply.started": "2022-02-05T11:09:41.70147Z",
     "shell.execute_reply": "2022-02-05T11:12:32.806455Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
